## Introduction

The llamabox-eval is a playground for evaluating the quality of summaries generated by llm.
The test is based on the following structure:

- a folder ```cases``` contains your test cases
- every test case contains a .json description file and optionally the full text of the content used for the summarization

Here is the structure of the decsription file
```json
{
    "caseid":"free form name",
    "prompt":"the prompt used to summarize",
    "fulltext":"",
    "fulltext_link":"./fulltext.txt",
    "human":"here the summary generated by a human being, the reference",
    "generated":"here the summary generated by your model",
    "bad":"here a summary that you use a bad example, mistaken summary",
    "benchmark":"here a summary from another model, that you use as a benchmark, eg. gpt4"
}
```

## Expected output

Here an example of output from the execution of **main.py**

<pre>
------- ROUGE 3 ---------
gen:0.0
ben:0.03508771929824561
bad:0.0
------- ROUGE L ---------
gen:0.20588235294117646
ben:0.20338983050847456
bad:0.0
------- BLEU ---------
gen:0
ben:0
bad:0
------- METEOR ---------
gen:0.24883499394969227
ben:0.26385005163760666
bad:0.00880281690140845
------- BERT ---------
gen:tensor([0.8384])
ben:tensor([0.8620])
bad:tensor([0.7829])
</pre>

